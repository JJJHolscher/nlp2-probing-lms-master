{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probing Language Models__\n",
    "\n",
    "This notebook serves as a start for your NLP2 assignment on probing Language Models. This notebook will become part of the contents that you will submit at the end, so make sure to keep your code (somewhat) clean :-)\n",
    "\n",
    "__note__: This is only the second time anyone is doing this assignment. That's exciting! But it might well be the case that certain aspects are too unclear. Do not hesitate at all to reach to me once you get stuck, I'd be grateful to help you out.\n",
    "\n",
    "__note 2__: This assignment is not dependent on big fancy GPUs. I run all this stuff on my own 3 year old CPU, without any Colab hassle. So it's up to you to decide how you want to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "For the Transformer models you are advised to make use of the `transformers` library of Huggingface: https://github.com/huggingface/transformers\n",
    "Their library is well documented, and they provide great tools to easily load in pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "## Your code for initializing the transformer model(s)\n",
    "#\n",
    "# Note that most transformer models use their own `tokenizer`, that should be loaded in as well.\n",
    "#\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "\n",
    "transformer_model = GPT2Model.from_pretrained('distilgpt2', output_hidden_states=True)\n",
    "transformer_tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', output_hidden_states=True)\n",
    "\n",
    "\n",
    "# Note that some models don't return the hidden states by default.\n",
    "# This can be configured by passing `output_hidden_states=True` to the `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Your code for initializing the rnn model(s)\n",
    "#\n",
    "# The Gulordava LSTM model can be found here: \n",
    "# https://drive.google.com/file/d/19Lp3AM4NEPycp_IBgoHfLc_V456pmUom/view?usp=sharing\n",
    "#\n",
    "# N.B: I have altered the RNNModel code to only output the hidden states that you are interested in.\n",
    "# If you want to do more experiments with this model you could have a look at the original code here:\n",
    "# https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py\n",
    "#\n",
    "from collections import defaultdict\n",
    "from lstm.model import RNNModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model_location = 'gulordava/state_dict.pt'  # <- point this to the location of the Gulordava .pt file\n",
    "lstm = RNNModel('LSTM', 50001, 650, 650, 2)\n",
    "lstm.load_state_dict(torch.load(model_location))\n",
    "\n",
    "\n",
    "# This LSTM does not use a Tokenizer like the Transformers, but a Vocab dictionary that maps a token to an id.\n",
    "with open('lstm/vocab.txt', encoding='ISO-8859-1') as f:\n",
    "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
    "\n",
    "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
    "vocab.update(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea that before you move on, you try to feed some text to your LMs; and check if everything works accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For this assignment you will train your probes on __treebank__ corpora. A treebank is a corpus that has been *parsed*, and stored in a representation that allows the parse tree to be recovered. Next to a parse tree, treebanks also often contain information about part-of-speech tags, which is exactly what we are after now.\n",
    "\n",
    "The treebank you will use for now is part of the Universal Dependencies project. I provide a sample of this treebank as well, so you can test your setup on that before moving on to larger amounts of data.\n",
    "\n",
    "Make sure you accustom yourself to the format that is created by the `conllu` library that parses the treebank files before moving on. For example, make sure you understand how you can access the pos tag of a token, or how to cope with the tree structure that is formed using the `to_tree()` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "from typing import List\n",
    "from conllu import parse_incr, TokenList\n",
    "\n",
    "\n",
    "# If stuff like `: str` and `-> ..` seems scary, fear not! \n",
    "# These are type hints that help you to understand what kind of argument and output is expected.\n",
    "def parse_corpus(filename: str) -> List[TokenList]:\n",
    "    '''Have every sentence become a TokenList.\n",
    "    \n",
    "    A token is a dictionary representing a word in a sentence. A token contains the following:\\n\n",
    "    <str> deprel: \\n\n",
    "    <list> deps: \\n\n",
    "    <dict> feats: {\n",
    "        <str> Definite: \\n\n",
    "        <str> PronType: \\n\n",
    "    }\n",
    "    <str> form: \\n\n",
    "    <int> head: \\n\n",
    "    <int> id: \\n\n",
    "    <str> lemma: \\n\n",
    "    <None> misc: \\n\n",
    "    <str> upos: \\n\n",
    "    <str> xpon: \\n\n",
    "    '''\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Representations\n",
    "\n",
    "We now have our data all set, our models are running and we are good to go!\n",
    "\n",
    "The next step is now to create the model representations for the sentences in our corpora. Once we have generated these representations we can store them, and train additional diagnostic (/probing) classifiers on top of the representations.\n",
    "\n",
    "There are a few things you should keep in mind here. Read these carefully, as these tips will save you a lot of time in your implementation.\n",
    "1. Transformer models make use of Byte-Pair Encodings (BPE), that chunk up a piece of next in subword pieces. For example, a word such as \"largely\" could be chunked up into \"large\" and \"ly\". We are interested in probing linguistic information on the __word__-level. Therefore, we will follow the suggestion of Hewitt et al. (2019a, footnote 4), and create the representation of a word by averaging over the representations of its subwords. So the representation of \"largely\" becomes the average of that of \"large\" and \"ly\".\n",
    "\n",
    "\n",
    "2. Subword chunks never overlap multiple tokens. In other words, say we have a phrase like \"None of the\", then the tokenizer might chunk that into \"No\"+\"ne\"+\" of\"+\" the\", but __not__ into \"No\"+\"ne o\"+\"f the\", as those chunks overlap multiple tokens. This is great for our setup! Otherwise it would have been quite challenging to distribute the representation of a subword over the 2 tokens it belongs to.\n",
    "\n",
    "\n",
    "3. **Important**: If you closely examine the provided treebank, you will notice that some tokens are split up into multiple pieces, that each have their own POS-tag. For example, in the first sentence the word \"Al-Zaman\" is split into \"Al\", \"-\", and \"Zaman\". In such cases, the conllu `TokenList` format will add the following attribute: `('misc', OrderedDict([('SpaceAfter', 'No')]))` to these tokens. Your model's tokenizer does not need to adhere to the same tokenization. E.g., \"Al-Zaman\" could be split into \"Al-\"+\"Za\"+\"man\", making it hard to match the representations with their correct pos-tag. Therefore I recommend you to not tokenize your entire sentence at once, but to do this based on the chunking of the treebank. <br /><br />\n",
    "Make sure to still incoporate the spaces in a sentence though, as these are part of the BPE of the tokenizer. That is, the tokenizer uses a different token id for `\"man\"`, than it does for `\" man\"`: the former could be part of `\" woman\"`=`\" wo`\"+`\"man\"`, whereas the latter would be the used in case *man* occurs at the start of a word. The tokenizer for GPT-2 adds spaces at the start of a token (represented as a `Ä ` symbol). This means that you should keep track whether the previous token had the `SpaceAfter` attribute set to `'No'`: in case it did not, you should manually prepend a `\" \"` ahead of the token.\n",
    "\n",
    "\n",
    "4. The LSTM LM does not have the issues related to subwords, but is far more restricted in its vocabulary. Make sure you keep the above points in mind though, when creating the LSTM representations. You might want to write separate functions for the LSTM, but that is up to you.\n",
    "\n",
    "\n",
    "5. **N.B.**: Make sure that when you run a sentence through your model, you do so within a `with torch.no_grad():` block, and that you have run `model.eval()` beforehand as well (to disable dropout).\n",
    "\n",
    "\n",
    "6. **N.B.**: Make sure to use a token's ``[\"form\"]`` attribute, and not the ``[\"lemma\"]``, as the latter will stem any relevant morphological information from the token. We don't want this, because we want to feed well-formed, grammatical sentences to our model.\n",
    "\n",
    "\n",
    "I would like to stress that if you feel hindered in any way by the simple code structure that is presented here, you are free to modify it :-) Just make sure it is clear to an outsider what you're doing, some helpful comments never hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH SENTENCE REPRESENTATIONS\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus, representation_size)\n",
    "# Make sure you correctly average the subword representations that belong to 1 token!\n",
    "\n",
    "def fetch_sen_reps(ud_parses: List[TokenList], model, tokenizer, concat=True) -> Tensor:\n",
    "    model = model.to(device)\n",
    "    # check which function to use\n",
    "    if isinstance(model, GPT2Model):\n",
    "        representations = fetch_sen_reps_transformer(ud_parses, model, tokenizer, concat)\n",
    "    else:\n",
    "        representations = fetch_sen_reps_lstm(ud_parses, model, tokenizer, concat)\n",
    "    \n",
    "    # return the representations\n",
    "    return representations\n",
    "\n",
    "def fetch_sen_reps_transformer(ud_parses: List[TokenList], model, tokenizer, concat_sentences) -> Tensor:\n",
    "    full_representation = []\n",
    "    \n",
    "    # loop over the parsed sentences\n",
    "    for sentence in ud_parses:\n",
    "        space_after = 'No'\n",
    "        sentence_ids = []\n",
    "        subword_dict = {}\n",
    "        chunk_offset = 0\n",
    "        \n",
    "        # loop over the words in the parsed sentence\n",
    "        for word_index, word in enumerate(sentence):\n",
    "            # check if the previous word had a space after\n",
    "            if (space_after == 'No'):\n",
    "                token = word['form']\n",
    "            else:\n",
    "                token = \" \" + word['form']\n",
    "            if (word['misc'] is not None):\n",
    "                if ('SpaceAfter' in word['misc']):\n",
    "                    space_after = word['misc']['SpaceAfter']\n",
    "                else:\n",
    "                    space_after = 'Yes'\n",
    "            else:\n",
    "                space_after = 'Yes'\n",
    "                \n",
    "            # tokenize the word\n",
    "            tokenized_word = tokenizer(token, return_tensors='pt')\n",
    "            subword_ids = tokenized_word['input_ids'].to(device)\n",
    "            \n",
    "            # check if multiple subwords\n",
    "            if (subword_ids.shape[1] > 1):\n",
    "                chunk_index_list = []\n",
    "                chunks = torch.chunk(subword_ids, subword_ids.shape[1], dim=1)\n",
    "                for chunk_index, chunk in enumerate(chunks):\n",
    "                    sentence_ids.append(chunk)\n",
    "                    chunk_index_list.append(word_index + chunk_index + chunk_offset)\n",
    "                chunk_offset += len(chunk_index_list) -1\n",
    "                subword_dict[word_index] = chunk_index_list\n",
    "            else:\n",
    "                subword_dict[word_index] = [word_index + chunk_offset]\n",
    "                sentence_ids.append(subword_ids)\n",
    "        \n",
    "        # pass the sentence through the model\n",
    "        sentence_ids = torch.cat(sentence_ids, dim=1)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sentence_representation = model(sentence_ids)\n",
    "            sentence_representation = sentence_representation.last_hidden_state.to(device)\n",
    "        \n",
    "        # mean the subwords into one word representation\n",
    "        sentence_representation = sentence_representation.squeeze(dim=0)\n",
    "        new_sentence_representation = [torch.mean(sentence_representation.index_select(0, torch.tensor(chunks, device=device)), dim=0)\n",
    "                                       for chunks in subword_dict.values()]\n",
    "        new_sentence_representation = torch.stack(new_sentence_representation, dim=0)\n",
    "        \n",
    "        # add to the list of sentence representations\n",
    "        full_representation.append(new_sentence_representation)\n",
    "    \n",
    "    # check whether to concat the sentences or not\n",
    "    if concat_sentences:\n",
    "        # concatenate the sentence representations into one big tensor\n",
    "        full_representation = torch.cat(full_representation, dim=0)\n",
    "    \n",
    "    # return the full representation\n",
    "    return full_representation\n",
    "\n",
    "def fetch_sen_reps_lstm(ud_parses: List[TokenList], model, tokenizer, concat_sentences) -> Tensor:\n",
    "    full_representation = []\n",
    "    \n",
    "    # loop over the parsed sentences\n",
    "    for sentence in ud_parses:\n",
    "        sentence_ids = []\n",
    "        \n",
    "        # loop over the words in the parsed sentence\n",
    "        for word in sentence:          \n",
    "            # tokenize the word\n",
    "            token_id = tokenizer[word['form']]\n",
    "            token_id = torch.tensor([[token_id]], device=device)\n",
    "            sentence_ids.append(token_id)\n",
    "            \n",
    "        # pass the sentence through the model\n",
    "        sentence_ids = torch.cat(sentence_ids, dim=1)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # generate a hidden state\n",
    "            hidden_state = model.init_hidden(sentence_ids.shape[0])\n",
    "            # take the representation of the sentence\n",
    "            sentence_representation = model(sentence_ids, hidden_state)\n",
    "        \n",
    "         # check if only 1 sentence\n",
    "        sentence_representation = sentence_representation.squeeze(dim=0)\n",
    "        if (len(ud_parses) == 1):\n",
    "            # return the sentence representation\n",
    "            return sentence_representation\n",
    "        else:\n",
    "            # add to the list of sentence representations\n",
    "            full_representation.append(sentence_representation)\n",
    "    \n",
    "    # check whether to concat the sentences or not\n",
    "    if concat_sentences:\n",
    "        # concatenate the sentence representations into one big tensor\n",
    "        full_representation = torch.cat(full_representation, dim=0)\n",
    "    \n",
    "    # return the full representation\n",
    "    return full_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate your activation extraction procedure I have set up the following assertion function as a sanity check. It compares your representation against a pickled version of mine. \n",
    "\n",
    "For this I used `distilgpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_msg(model_name, gold_embs, embs, i2w):\n",
    "    with open(f'{model_name}_tokens1.pickle', 'rb') as f:\n",
    "        sen_tokens = pickle.load(f)\n",
    "        \n",
    "    diff = torch.abs(embs - gold_embs)\n",
    "    max_diff = torch.max(diff)\n",
    "    avg_diff = torch.mean(diff)\n",
    "    \n",
    "    print(f\"{model_name} embeddings don't match!\")\n",
    "    print(f\"Max diff.: {max_diff:.4f}\\nMean diff. {avg_diff:.4f}\")\n",
    "\n",
    "    print(\"\\nCheck if your tokenization matches with the original tokenization:\")\n",
    "    for idx in sen_tokens.squeeze():\n",
    "        if isinstance(i2w, list):\n",
    "            token = i2w[idx]\n",
    "        else:\n",
    "            token = i2w.convert_ids_to_tokens(idx.item())\n",
    "        print(f\"{idx:<6} {token}\")\n",
    "\n",
    "\n",
    "def assert_sen_reps(model, tokenizer, lstm, vocab):\n",
    "    with open('distilgpt2_emb1.pickle', 'rb') as f:\n",
    "        distilgpt2_emb1 = pickle.load(f).to(device)\n",
    "        \n",
    "    with open('lstm_emb1.pickle', 'rb') as f:\n",
    "        lstm_emb1 = pickle.load(f).to(device)\n",
    "    \n",
    "    corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')[:1]\n",
    "    \n",
    "    own_distilgpt2_emb1 = fetch_sen_reps(corpus, model, tokenizer)\n",
    "    own_lstm_emb1 = fetch_sen_reps(corpus, lstm, vocab)\n",
    "    \n",
    "    assert distilgpt2_emb1.shape == own_distilgpt2_emb1.shape, \\\n",
    "        f\"Distilgpt2 shape mismatch: {distilgpt2_emb1.shape} (gold) vs. {own_distilgpt2_emb1.shape} (yours)\"\n",
    "    assert lstm_emb1.shape == own_lstm_emb1.shape, \\\n",
    "        f\"LSTM shape mismatch: {lstm_emb1.shape} (gold) vs. {own_lstm_emb1.shape} (yours)\"\n",
    "\n",
    "    if not torch.allclose(distilgpt2_emb1, own_distilgpt2_emb1, rtol=1e-3, atol=1e-3):\n",
    "        error_msg(\"distilgpt2\", distilgpt2_emb1, own_distilgpt2_emb1, tokenizer)\n",
    "    if not torch.allclose(lstm_emb1, own_lstm_emb1, rtol=1e-3, atol=1e-3):\n",
    "        error_msg(\"lstm\", lstm_emb1, own_lstm_emb1, list(vocab.keys()))\n",
    "\n",
    "\n",
    "assert_sen_reps(transformer_model, transformer_tokenizer, lstm, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should define a function that extracts the corresponding POS labels for each activation, which we do based on the **``\"upostag\"``** attribute of a token (so not the ``xpostag`` attribute). These labels will be transformed to a tensor containing the label index for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH POS LABELS\n",
    "\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
    "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
    "def fetch_pos_tags(ud_parses: List[TokenList], pos_vocab=None) -> Tensor:\n",
    "    # check if the vocabulary is None\n",
    "    if pos_vocab is None:\n",
    "        # create a new vocab\n",
    "        pos_vocab = defaultdict()\n",
    "        \n",
    "        # word index is 0\n",
    "        word_index = 0\n",
    "    else:\n",
    "        # word index is the length of the vocab\n",
    "        word_index = len(pos_vocab)\n",
    "    \n",
    "    # create a list of tags from the input\n",
    "    all_tags = []\n",
    "    \n",
    "    # loop over the parsed sentences\n",
    "    for sentence in ud_parses:    \n",
    "        # loop over the words in the parsed sentence\n",
    "        for word in sentence:\n",
    "            # get the tag\n",
    "            tag = word['upostag']\n",
    "            \n",
    "            # check if the tag does not exist in the vocabulary\n",
    "            if tag not in pos_vocab:\n",
    "                # create a new entry\n",
    "                pos_vocab[tag] = word_index\n",
    "                \n",
    "                # add to the word index\n",
    "                word_index += 1\n",
    "            \n",
    "            # add tag to the tags list\n",
    "            all_tags.append(torch.tensor(pos_vocab[tag], device=device))\n",
    "    \n",
    "    # stack the tags into a tensor\n",
    "    all_tags = torch.stack(all_tags, dim=0)\n",
    "    \n",
    "    # return the tags and vocabulary\n",
    "    return all_tags, pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function that combines the previous functions, and creates 2 tensors for a .conllu file: \n",
    "# 1 containing the token representations, and 1 containing the (tokenized) pos_tags.\n",
    "\n",
    "def create_data(filename: str, lm, w2i, pos_vocab=None, cut_off=None):\n",
    "    ud_parses = parse_corpus(filename)\n",
    "    if cut_off is not None:\n",
    "        ud_parses = ud_parses[:cut_off]\n",
    "    \n",
    "    sen_reps = fetch_sen_reps(ud_parses, lm, w2i)\n",
    "    pos_tags, pos_vocab = fetch_pos_tags(ud_parses, pos_vocab=pos_vocab)\n",
    "    \n",
    "    return sen_reps, pos_tags, pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = transformer_model  # or `lstm`\n",
    "index = transformer_tokenizer  # or `vocab`\n",
    "use_sample = True\n",
    "\n",
    "train_x, train_y, train_vocab = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "    lm, \n",
    "    index\n",
    ")\n",
    "\n",
    "dev_x, dev_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "    lm, \n",
    "    index,\n",
    "    pos_vocab=train_vocab\n",
    ")\n",
    "\n",
    "test_x, test_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "    lm,\n",
    "    index,\n",
    "    pos_vocab=train_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Classification\n",
    "\n",
    "We now have our models, our data, _and_ our representations all set! Hurray, well done. We can finally move onto the cool stuff, i.e. training the diagnostic classifiers (DCs).\n",
    "\n",
    "DCs are simple in their complexity on purpose. To read more about why this is the case you could already have a look at the \"Designing and Interpreting Probes with Control Tasks\" by Hewitt and Liang (esp. Sec. 3.2).\n",
    "\n",
    "A simple linear classifier will suffice for now, don't bother with adding fancy non-linearities to it.\n",
    "\n",
    "I am personally a fan of the `skorch` library, that provides `sklearn`-like functionalities for training `torch` models, but you are free to train your dc using whatever method you prefer.\n",
    "\n",
    "As this is an Artificial Intelligence master and you have all done ML1 + DL, I expect you to use your train/dev/test splits correctly ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "class DiagnosticClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size, hidden_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # initialize the layers of the model\n",
    "        modules = []\n",
    "        for hidden_size in hidden_sizes:\n",
    "            modules.append(torch.nn.Linear(in_size, hidden_size))\n",
    "            in_size = hidden_size\n",
    "        modules.append(torch.nn.Linear(in_size, out_size))\n",
    "        self.layers = torch.nn.Sequential(*modules)\n",
    "            \n",
    "        # add softmax\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass x through the model\n",
    "        out = self.layers(x)\n",
    "        \n",
    "        # run through the softmax\n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        # return the output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "\n",
    "# function to train the diagnostic classifier\n",
    "def train_dc(diagnostic_classifier, train_x, train_y, dev_x, dev_y, device, batch_size=24, max_epochs=40, lr=3e-4, hidden_sizes=[]):\n",
    "    # create the validation dataset\n",
    "    val_dataset = Dataset(dev_x, dev_y)\n",
    "    train_dataset = Dataset(train_x, train_y)\n",
    "    \n",
    "    # initialize the model\n",
    "    net = NeuralNetClassifier(\n",
    "        diagnostic_classifier,\n",
    "        module__in_size=train_x[0].shape[0],\n",
    "        module__out_size=train_y.shape[0],\n",
    "        module__hidden_sizes=hidden_sizes,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer = torch.optim.Adam,\n",
    "        optimizer__lr=lr,\n",
    "        iterator_train__shuffle=True,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        train_split=predefined_split(val_dataset),\n",
    "        device=device,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='valid_loss',\n",
    "                         patience=3,\n",
    "                         threshold=0.0001,\n",
    "                         threshold_mode='rel',\n",
    "                         lower_is_better=True)\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # train the model\n",
    "    net.fit(train_dataset, None)\n",
    "    \n",
    "    # return the model\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-57b41f9dea8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train the diagnostic classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrained_dc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDiagnosticClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# train the diagnostic classifier\n",
    "trained_dc = train_dc(DiagnosticClassifier, train_x[:30000].cpu(), train_y[:30000].cpu(), dev_x.cpu(), dev_y.cpu(), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees\n",
    "\n",
    "For our gold labels, we need to recover the node distances from our parse tree. For this we will use the functionality provided by `ete3`, that allows us to compute that directly. I have provided code that transforms a `TokenTree` to a `Tree` in `ete3` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to transform your conllu tree to an nltk.Tree, for better visualisation\n",
    "\n",
    "def rec_tokentree_to_nltk(tokentree):\n",
    "    token = tokentree.token[\"form\"]\n",
    "    tree_str = f\"({token} {' '.join(rec_tokentree_to_nltk(t) for t in tokentree.children)})\"\n",
    "\n",
    "    return tree_str\n",
    "\n",
    "\n",
    "def tokentree_to_nltk(tokentree):\n",
    "    from nltk import Tree as NLTKTree\n",
    "\n",
    "    tree_str = rec_tokentree_to_nltk(tokentree)\n",
    "\n",
    "    return NLTKTree.fromstring(tree_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ete3\n",
    "from ete3 import Tree as EteTree\n",
    "\n",
    "\n",
    "class FancyTree(EteTree):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, format=1, **kwargs)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.get_ascii(show_internal=True)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "def rec_tokentree_to_ete(tokentree):\n",
    "    idx = str(tokentree.token[\"id\"])\n",
    "    children = tokentree.children\n",
    "    if children:\n",
    "        return f\"({','.join(rec_tokentree_to_ete(t) for t in children)}){idx}\"\n",
    "    else:\n",
    "        return idx\n",
    "    \n",
    "def tokentree_to_ete(tokentree):\n",
    "    newick_str = rec_tokentree_to_ete(tokentree)\n",
    "\n",
    "    return FancyTree(f\"{newick_str};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   /-2\n",
      "  |\n",
      "  |--3\n",
      "  |\n",
      "  |--4\n",
      "  |\n",
      "  |   /6 /-5\n",
      "  |  |\n",
      "  |  |   /-9\n",
      "  |  |  |\n",
      "  |  |  |--10\n",
      "  |  |  |\n",
      "  |  |  |--11\n",
      "  |  |-8|\n",
      "  |  |  |--12\n",
      "  |-7|  |\n",
      "  |  |  |--13\n",
      "  |  |  |\n",
      "  |  |   \\15/-14\n",
      "-1|  |\n",
      "  |  |   /-16\n",
      "  |  |  |\n",
      "  |  |  |--17\n",
      "  |  |  |\n",
      "  |   \\18   /-19\n",
      "  |     |  |\n",
      "  |     |  |--20\n",
      "  |     |  |\n",
      "  |     |  |-23/-22\n",
      "  |      \\21\n",
      "  |        |--24\n",
      "  |        |\n",
      "  |        |   /-25\n",
      "  |        |  |\n",
      "  |         \\28--26\n",
      "  |           |\n",
      "  |            \\-27\n",
      "  |\n",
      "   \\-29\n"
     ]
    }
   ],
   "source": [
    "# Let's check if it works!\n",
    "# We can read in a corpus using the code that was already provided, and convert it to an ete3 Tree.\n",
    "\n",
    "def parse_corpus(filename):\n",
    "    from conllu import parse_incr\n",
    "\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses\n",
    "\n",
    "corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')\n",
    "item = corpus[0]\n",
    "tokentree = item.to_tree()\n",
    "ete3_tree = tokentree_to_ete(tokentree)\n",
    "print(ete3_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we label a token by its token id (converted to a string). Based on these id's we are going to retrieve the node distances.\n",
    "\n",
    "To create the true distances of a parse tree in our treebank, we are going to use the `.get_distance` method that is provided by `ete3`: http://etetoolkit.org/docs/latest/tutorial/tutorial_trees.html#working-with-branch-distances\n",
    "\n",
    "We will store all these distances in a `torch.Tensor`.\n",
    "\n",
    "Please fill in the gap in the following method. I recommend you to have a good look at Hewitt's blog post  about these node distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_distances(corpus):\n",
    "    all_distances = []\n",
    "\n",
    "    for item in (corpus):\n",
    "        tokentree = item.to_tree()\n",
    "        ete_tree = tokentree_to_ete(tokentree)\n",
    "\n",
    "        sen_len = len(ete_tree.search_nodes())\n",
    "        distances = torch.zeros((sen_len, sen_len), device=device)\n",
    "\n",
    "        # Your code for computing all the distances comes here.\n",
    "        \n",
    "        # loop over the nodes in the tree\n",
    "        for index_1, node_1 in enumerate(ete_tree.traverse(\"postorder\")):\n",
    "            node_distance = torch.zeros((1, sen_len), device=device)\n",
    "            # get the distance of the node to all other nodes\n",
    "            for index_2, node_2 in enumerate(ete_tree.traverse(\"postorder\")):\n",
    "                distance = node_1.get_distance(node_2.name)\n",
    "                node_distance[0][index_2] = distance\n",
    "            distances[index_1] = node_distance\n",
    "\n",
    "        all_distances.append(distances)\n",
    "\n",
    "    return all_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is now to do the previous step the other way around. After all, we are mainly interested in predicting the node distances of a sentence, in order to recreate the corresponding parse tree.\n",
    "\n",
    "Hewitt et al. reconstruct a parse tree based on a _minimum spanning tree_ (MST, https://en.wikipedia.org/wiki/Minimum_spanning_tree). Fortunately for us, we can simply import a method from `scipy` that retrieves this MST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_mst(distances):\n",
    "    distances = torch.triu(distances).cpu().detach().numpy()\n",
    "    \n",
    "    mst = minimum_spanning_tree(distances).toarray()\n",
    "    mst[mst>0] = 1.\n",
    "    \n",
    "    return mst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what this looks like, by looking at a relatively short sentence in the sample corpus.\n",
    "\n",
    "If your addition to the `create_gold_distances` method has been correct, you should be able to run the following snippet. This then shows you the original parse tree, the distances between the nodes, and the MST that is retrieved from these distances. Can you spot the edges in the MST matrix that correspond to the edges in the parse tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   /2 /-1\n",
      "  |\n",
      "  |--3\n",
      "  |\n",
      "  |--4\n",
      "  |\n",
      "  |   /-6\n",
      "  |  |\n",
      "-5|  |--7\n",
      "  |-8|\n",
      "  |  |   /-9\n",
      "  |  |  |\n",
      "  |   \\12--10\n",
      "  |     |\n",
      "  |      \\-11\n",
      "  |\n",
      "   \\-13 \n",
      "\n",
      "tensor([[0., 1., 3., 3., 4., 4., 5., 5., 5., 4., 3., 3., 2.],\n",
      "        [1., 0., 2., 2., 3., 3., 4., 4., 4., 3., 2., 2., 1.],\n",
      "        [3., 2., 0., 2., 3., 3., 4., 4., 4., 3., 2., 2., 1.],\n",
      "        [3., 2., 2., 0., 3., 3., 4., 4., 4., 3., 2., 2., 1.],\n",
      "        [4., 3., 3., 3., 0., 2., 3., 3., 3., 2., 1., 3., 2.],\n",
      "        [4., 3., 3., 3., 2., 0., 3., 3., 3., 2., 1., 3., 2.],\n",
      "        [5., 4., 4., 4., 3., 3., 0., 2., 2., 1., 2., 4., 3.],\n",
      "        [5., 4., 4., 4., 3., 3., 2., 0., 2., 1., 2., 4., 3.],\n",
      "        [5., 4., 4., 4., 3., 3., 2., 2., 0., 1., 2., 4., 3.],\n",
      "        [4., 3., 3., 3., 2., 2., 1., 1., 1., 0., 1., 3., 2.],\n",
      "        [3., 2., 2., 2., 1., 1., 2., 2., 2., 1., 0., 2., 1.],\n",
      "        [3., 2., 2., 2., 3., 3., 4., 4., 4., 3., 2., 0., 1.],\n",
      "        [2., 1., 1., 1., 2., 2., 3., 3., 3., 2., 1., 1., 0.]], device='cuda:0') \n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "item = corpus[5]\n",
    "tokentree = item.to_tree()\n",
    "ete3_tree = tokentree_to_ete(tokentree)\n",
    "print(ete3_tree, '\\n')\n",
    "\n",
    "gold_distance = create_gold_distances(corpus[5:6])[0]\n",
    "print(gold_distance, '\\n')\n",
    "\n",
    "mst = create_mst(gold_distance)\n",
    "print(mst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to map edge distances back to parse trees, we can create code for our quantitative evaluation. For this we will use the Undirected Unlabeled Attachment Score (UUAS), which is expressed as:\n",
    "\n",
    "$$\\frac{\\text{number of predicted edges that are an edge in the gold parse tree}}{\\text{number of edges in the gold parse tree}}$$\n",
    "\n",
    "To do this, we will need to obtain all the edges from our MST matrix. Note that, since we are using undirected trees, that an edge can be expressed in 2 ways: an edge between node $i$ and node $j$ is denoted by both `mst[i,j] = 1`, or `mst[j,i] = 1`.\n",
    "\n",
    "You will write code that computes the UUAS score for a matrix of predicted distances, and the corresponding gold distances. I recommend you to split this up into 2 methods: 1 that retrieves the edges that are present in an MST matrix, and one general method that computes the UUAS score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges(mst):\n",
    "    edges = set()\n",
    "\n",
    "    # Your code for retrieving the edges from the MST matrix\n",
    "    \n",
    "    # create tuples for all instances where mst is 1\n",
    "    for row_index, row in enumerate(mst):\n",
    "        for column_index, column in enumerate(row):\n",
    "            if column == 1:\n",
    "                edges.add((row_index, column_index))\n",
    "                edges.add((column_index, row_index))\n",
    "\n",
    "    return edges\n",
    "\n",
    "def calc_uuas(pred_distances, gold_distances):\n",
    "    uuas = []\n",
    "    \n",
    "    # Your code for computing the UUAS score\n",
    "    \n",
    "    # check the number of dimensions given (if batches are supplied)\n",
    "    if (len(pred_distances.shape) > 2):\n",
    "        pred_distances_chunks = torch.chunk(pred_distances, pred_distances.shape[0], dim=0)\n",
    "        gold_distances_chunks = torch.chunk(gold_distances, gold_distances.shape[0], dim=0)\n",
    "    else:\n",
    "        pred_distances_chunks = [pred_distances]\n",
    "        gold_distances_chunks = [gold_distances]\n",
    "        \n",
    "    # loop over all chunks\n",
    "    for index, pred_distance in enumerate(pred_distances_chunks):\n",
    "        # get the accompanying gold distance\n",
    "        gold_distance = gold_distances_chunks[index]\n",
    "        \n",
    "        # get edges of the predicted distances\n",
    "        if (len(pred_distance.shape) > 2):\n",
    "            pred_distance = pred_distance.squeeze(dim=0)\n",
    "        pred_mst = create_mst(pred_distance)\n",
    "        pred_edges = edges(pred_mst)\n",
    "    \n",
    "        # get edges of the gold distances\n",
    "        if (len(gold_distance.shape) > 2):\n",
    "            gold_distance = gold_distance.squeeze(dim=0)\n",
    "        gold_mst = create_mst(gold_distance)\n",
    "        gold_edges = edges(gold_mst)\n",
    "    \n",
    "        # count the occurences of the predicted edges in the gold edges\n",
    "        count_pred_in_gold = 0\n",
    "        for edge in pred_edges:\n",
    "            if edge in gold_edges:\n",
    "                count_pred_in_gold += 1\n",
    "    \n",
    "        # calculate the uuas\n",
    "        if (len(gold_edges) == 0):\n",
    "            uuas_score = 0\n",
    "        else:\n",
    "            uuas_score = count_pred_in_gold / len(gold_edges)\n",
    "        uuas.append(float(uuas_score))\n",
    "    \n",
    "    # calculate the mean uuas\n",
    "    mean_uuas = np.mean(uuas)\n",
    "    \n",
    "    # return the mean uuas\n",
    "    return mean_uuas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Probes\n",
    "\n",
    "We now have everything in place to start doing the actual exciting stuff: training our structural probe!\n",
    "    \n",
    "To make life easier for you, we will simply take the `torch` code for this probe from John Hewitt's repository. This allows you to focus on the training regime from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class StructuralProbe(nn.Module):\n",
    "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
    "    For a batch of sentences, computes all n^2 pairs of distances\n",
    "    for each sentence in the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.probe_rank = rank\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
    "        \n",
    "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\" Computes all n^2 pairs of distances after projection\n",
    "        for each sentence in a batch.\n",
    "        Note that due to padding, some distances will be non-zero for pads.\n",
    "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
    "        Args:\n",
    "          batch: a batch of word representations of the shape\n",
    "            (batch_size, max_seq_len, representation_dim)\n",
    "        Returns:\n",
    "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
    "        \"\"\"\n",
    "        transformed = torch.matmul(batch, self.proj)\n",
    "        \n",
    "        batchlen, seqlen, rank = transformed.size()\n",
    "        \n",
    "        transformed = transformed.unsqueeze(2)\n",
    "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
    "        transposed = transformed.transpose(1,2)\n",
    "        \n",
    "        diffs = transformed - transposed\n",
    "        \n",
    "        squared_diffs = diffs.pow(2)\n",
    "        squared_distances = torch.sum(squared_diffs, -1)\n",
    "\n",
    "        return squared_distances\n",
    "\n",
    "    \n",
    "class L1DistanceLoss(nn.Module):\n",
    "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, label_batch, length_batch):\n",
    "        \"\"\" Computes L1 loss on distance matrices.\n",
    "        Ignores all entries where label_batch=-1\n",
    "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
    "        and then across the batch.\n",
    "        Args:\n",
    "          predictions: A pytorch batch of predicted distances\n",
    "          label_batch: A pytorch batch of true distances\n",
    "          length_batch: A pytorch batch of sentence lengths\n",
    "        Returns:\n",
    "          A tuple of:\n",
    "            batch_loss: average loss in the batch\n",
    "            total_sents: number of sentences in the batch\n",
    "        \"\"\"\n",
    "        labels_1s = (label_batch != -1).float()\n",
    "        predictions_masked = predictions * labels_1s\n",
    "        labels_masked = label_batch * labels_1s\n",
    "        total_sents = torch.sum((length_batch != 0)).float()\n",
    "        squared_lengths = length_batch.pow(2).float()\n",
    "\n",
    "        if total_sents > 0:\n",
    "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
    "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
    "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
    "        \n",
    "        else:\n",
    "            batch_loss = torch.tensor(0.0)\n",
    "        \n",
    "        return batch_loss, total_sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have provided a rough outline for the training regime that you can use. Note that the hyper parameters that I provide here only serve as an indication, but should be (briefly) explored by yourself.\n",
    "\n",
    "As can be seen in Hewitt's code above, there exists functionality in the probe to deal with batched input. It is up to you to use that: a (less efficient) method can still incorporate batches by doing multiple forward passes for a batch and computing the backward pass only once for the summed losses of all these forward passes. (_I know, this is not the way to go, but in the interest of time that is allowed ;-), the purpose of the assignment is writing a good paper after all_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Similar to the `create_data` method of the previous notebook, I recommend you to use a method \n",
    "that initialises all the data of a corpus. Note that for your embeddings you can use the \n",
    "`fetch_sen_reps` method again. However, for the POS probe you concatenated all these representations into \n",
    "1 big tensor of shape (num_tokens_in_corpus, model_dim). \n",
    "\n",
    "The StructuralProbe expects its input to contain all the representations of 1 sentence, so I recommend you\n",
    "to update your `fetch_sen_reps` method in a way that it is easy to retrieve all the representations that \n",
    "correspond to a single sentence.\n",
    "''' \n",
    "\n",
    "def init_corpus(path, model, tokenizer, concat=False, cutoff=None):\n",
    "    \"\"\" Initialises the data of a corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to corpus location\n",
    "    concat : bool, optional\n",
    "        Optional toggle to concatenate all the tensors\n",
    "        returned by `fetch_sen_reps`.\n",
    "    cutoff : int, optional\n",
    "        Optional integer to \"cutoff\" the data in the corpus.\n",
    "        This allows only a subset to be used, alleviating \n",
    "        memory usage.\n",
    "    \"\"\"\n",
    "    corpus = parse_corpus(path)[:cutoff]\n",
    "\n",
    "    embs = fetch_sen_reps(corpus, model, tokenizer, concat=concat)    \n",
    "    gold_distances = create_gold_distances(corpus)\n",
    "    \n",
    "    return gold_distances, embs\n",
    "\n",
    "\n",
    "# I recommend you to write a method that can evaluate the UUAS & loss score for the dev (& test) corpus.\n",
    "# Feel free to alter the signature of this method.\n",
    "def evaluate_probe(probe, data_x, data_y, batch_size):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # create a loss function\n",
    "    loss_function =  L1DistanceLoss()\n",
    "    \n",
    "    loss = []\n",
    "    uuas = []\n",
    "    \n",
    "    # loop over the data\n",
    "    for i in range(0, len(data_x), batch_size):\n",
    "        # get the batch\n",
    "        batch_x = data_x[i : (i + batch_size)]\n",
    "        batch_y = data_y[i : (i + batch_size)]\n",
    "        \n",
    "        # calculate the sentence lengths\n",
    "        sentence_lengths = torch.tensor([i.shape[0] for i in batch_x])\n",
    "            \n",
    "        # pad the batch\n",
    "        batch_x = torch.nn.utils.rnn.pad_sequence(batch_x, batch_first=True)\n",
    "        max_batch_y = max([i.shape[0] for i in batch_y])\n",
    "        batch_y = [torch.nn.functional.pad(i, (0, (max_batch_y - i.shape[0]), 0, 0)) for i in batch_y]\n",
    "        batch_y = torch.nn.utils.rnn.pad_sequence(batch_y, batch_first=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # pass the batch through the probe\n",
    "            squared_distances = probe(batch_x)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss_score, total_sents = loss_function(squared_distances, batch_y.to(device), sentence_lengths.to(device))\n",
    "            loss.append(loss_score / torch.tensor(batch_size, device=device))\n",
    "            \n",
    "            # calculate the uuas score\n",
    "            uuas_score = calc_uuas(squared_distances, batch_y.to(device))\n",
    "            uuas.append(uuas_score)\n",
    "    \n",
    "    # sum the loss\n",
    "    loss = torch.sum(torch.stack(loss, dim=0), dim=0)\n",
    "    \n",
    "    # mean the uuas\n",
    "    uuas = np.mean(uuas)\n",
    "    \n",
    "    # return the loss and uuas\n",
    "    return loss, uuas\n",
    "\n",
    "# Feel free to alter the signature of this method.\n",
    "def train_probe(train_x, train_y, dev_x, dev_y, test_x, test_y, emb_dim=768, batch_size=24, epochs=10):\n",
    "    rank = 64\n",
    "    lr = 0.01\n",
    "\n",
    "    probe = StructuralProbe(emb_dim, rank).to(device)\n",
    "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "    loss_function =  L1DistanceLoss()\n",
    "    \n",
    "    # performance measures per epoch\n",
    "    train_loss_epochs = []\n",
    "    dev_loss_epochs = []\n",
    "    dev_uuas_epochs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "            \n",
    "        for i in range(0, len(train_x), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # YOUR CODE FOR DOING A PROBE FORWARD PASS\n",
    "            \n",
    "            # get the batch\n",
    "            batch_x = train_x[i : (i + batch_size)]\n",
    "            batch_y = train_y[i : (i + batch_size)]\n",
    "            \n",
    "            # calculate the sentence lengths\n",
    "            sentence_lengths = torch.tensor([i.shape[0] for i in batch_x])\n",
    "            \n",
    "            # pad the batch\n",
    "            batch_x = torch.nn.utils.rnn.pad_sequence(batch_x, batch_first=True, padding_value=-1.0)\n",
    "            max_batch_y = max([i.shape[0] for i in batch_y])\n",
    "            batch_y = [torch.nn.functional.pad(i, (0, (max_batch_y - i.shape[0]), 0, 0), value=-1.0) for i in batch_y]\n",
    "            batch_y = torch.nn.utils.rnn.pad_sequence(batch_y, batch_first=True, padding_value=-1.0)\n",
    "            \n",
    "            # pass the batch through the probe\n",
    "            squared_distances = probe(batch_x.to(device))\n",
    "            \n",
    "            # calculate the loss\n",
    "            batch_loss, total_sents = loss_function(squared_distances, batch_y.to(device), sentence_lengths.to(device))\n",
    "            batch_loss = batch_loss / torch.tensor(batch_size, device=device)\n",
    "            train_loss.append(batch_loss)\n",
    "            \n",
    "            # backward on the loss and set a step\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        dev_loss, dev_uuas = evaluate_probe(probe, dev_x, dev_y, batch_size)\n",
    "        \n",
    "        # update user on dev loss and uuas after epoch\n",
    "        train_loss = torch.sum(torch.stack(train_loss, dim=0), dim=0)\n",
    "        print(f'Epoch {epoch:d} finished: train_loss={train_loss:.4f} & valid_loss={dev_loss:.4f} & valid_uuas={dev_uuas:.4f}')\n",
    "\n",
    "        # Using a scheduler is up to you, and might require some hyper param fine-tuning\n",
    "        scheduler.step(dev_loss)\n",
    "        \n",
    "        # add the performance measures to the lists\n",
    "        train_loss_epochs.append(train_loss)\n",
    "        dev_loss_epochs.append(dev_loss)\n",
    "        dev_uuas_epochs.append(dev_uuas)\n",
    "\n",
    "    test_loss, test_uuas = evaluate_probe(probe, test_x, test_y, batch_size)\n",
    "    \n",
    "    # update user on test loss and uuas after training\n",
    "    print(f'Testing finished: test_loss={test_loss:.4f} & test_uuas={test_uuas:.4f}')\n",
    "    \n",
    "    # return the trained probe and performance measures\n",
    "    return probe, train_loss_epochs, dev_loss_epochs, dev_uuas_epochs, test_loss, test_uuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "# emb_dim of 650 for lstm\n",
    "train_y, train_x = init_corpus('data/en_ewt-ud-train.conllu', transformer_model, transformer_tokenizer) # lstm and vocab for lstm\n",
    "dev_y, dev_x = init_corpus('data/en_ewt-ud-dev.conllu', transformer_model, transformer_tokenizer)\n",
    "test_y, test_x = init_corpus('data/en_ewt-ud-test.conllu', transformer_model, transformer_tokenizer)\n",
    "\n",
    "# train the probe\n",
    "trained_probe, _, _, _, _, _ = train_probe(train_x, train_y, dev_x, dev_y, test_x, test_y, emb_dim=768, batch_size=24, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of model size on probing results\n",
    "In this section we introduce 2 new Transformer models. Namely, base GPT2 and GPT2-Medium. We will use these to analyze whether a larger model (larger number of parameters) has a positive influence on probing results. Meaning that models would be better able to capture linguistic and syntactic features from text in their hidden states. Thus, we compare the following models:\n",
    "* distilgpt2: 6-layer, 768-hidden, 12-heads, 82M parameters\n",
    "* gpt2: 12-layer, 768-hidden, 12-heads, 117M parameters\n",
    "* gpt2-medium: 24-layer, 1024-hidden, 16-heads, 345M parameters\n",
    "    \n",
    "In the following codeblocks we will introduce the 2 new Transformer models. The conclusion of the comparison will be included in the conclusions of the experiments in the other sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gpt_base_model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)\n",
    "gpt_base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gpt_medium_model = GPT2Model.from_pretrained('gpt2-medium', output_hidden_states=True)\n",
    "gpt_medium_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extent of POS-tags encoding\n",
    "In this section we will examine to what extent the GPT-2 transformer and Gulordava LSTM encode linguistic features. We do this by examining to what extent they encode POS-tags.\n",
    "    \n",
    "We start by training a diagnostic classifier on the encodings from the GPT-2 transformer model and training a diagnostic classifier on the encodings from the Gulordava LSTM model. We then compare the performance of the classifier on both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a function that creates the correct data for each of the models and a function that trains the Diagnostic Classifier on the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the correct data for each of the models\n",
    "def create_dc_data(model_used='Transformer'):\n",
    "    # check which model to use\n",
    "    if (model_used == 'Transformer'):\n",
    "        lm = transformer_model\n",
    "        w2i = transformer_tokenizer\n",
    "    elif (model_used == 'GPT2_Base'):\n",
    "        lm = gpt_base_model\n",
    "        w2i = gpt_base_tokenizer\n",
    "    elif (model_used == 'GPT2_Medium'):\n",
    "        lm = gpt_medium_model\n",
    "        w2i = gpt_medium_tokenizer\n",
    "    else:\n",
    "        lm = lstm\n",
    "        w2i = vocab\n",
    "    use_sample = False\n",
    "\n",
    "    train_x, train_y, train_vocab = create_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "        lm, \n",
    "        w2i,\n",
    "        cut_off=30000\n",
    "    )\n",
    "\n",
    "    dev_x, dev_y, _ = create_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "        lm, \n",
    "        w2i,\n",
    "        pos_vocab=train_vocab\n",
    "    )\n",
    "\n",
    "    test_x, test_y, _ = create_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "        lm,\n",
    "        w2i,\n",
    "        pos_vocab=train_vocab\n",
    "    )\n",
    "    \n",
    "    # return the data\n",
    "    return train_x, train_y, dev_x, dev_y, test_x, test_y, len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "\n",
    "# function that trains the DC and returns the metrics\n",
    "def train_test_dc(model_used='Transformer', batch_size=24, max_epochs=200, save_results=True):\n",
    "    # get the data for the model\n",
    "    print('Loading data..')\n",
    "    train_x, train_y, dev_x, dev_y, test_x, test_y, vocab_length = create_dc_data(model_used)\n",
    "    print('Data loaded')\n",
    "    \n",
    "    # combine train and validation datasets\n",
    "    x_train = torch.cat((train_x, dev_x), dim=0)\n",
    "    y_train = torch.cat((train_y, dev_y), dim=0)\n",
    "    \n",
    "    # train the diagnostic classifier\n",
    "    print('Starting training..')\n",
    "    trained_dc = train_dc(DiagnosticClassifier, train_x[:30000].cpu(), \n",
    "                          train_y[:30000].cpu(), dev_x.cpu(), dev_y.cpu(), \n",
    "                          device, batch_size=batch_size, max_epochs=max_epochs)\n",
    "    print('Training finished')\n",
    "    \n",
    "    # get the DC data\n",
    "    train_loss = trained_dc.history[:, 'train_loss']\n",
    "    valid_loss = trained_dc.history[:, 'valid_loss']\n",
    "    valid_acc = trained_dc.history[:, 'valid_acc']\n",
    "    \n",
    "    # run the test data through the model\n",
    "    test_preds = trained_dc.predict(test_x.cpu())\n",
    "    test_acc = accuracy_score(test_y.cpu(), test_preds)\n",
    "    \n",
    "    if save_results:\n",
    "        # save the results\n",
    "        print('Saving results..')\n",
    "        data = {}\n",
    "        data['train_loss'] = train_loss\n",
    "        data['valid_loss'] = valid_loss\n",
    "        data['valid_acc'] = valid_acc\n",
    "        data['test_acc'] = test_acc\n",
    "        with open('model_results/' + model_used.lower() + '_dc.txt', 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        print('Results saved')\n",
    "    \n",
    "    # return the measures \n",
    "    return train_loss, valid_loss, valid_acc, test_acc, vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the diagnostic classifier on the data of both models in the following code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded\n",
      "Starting training..\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.2787\u001b[0m       \u001b[32m0.8157\u001b[0m        \u001b[35m0.7255\u001b[0m  10.7360\n",
      "      2        \u001b[36m0.4316\u001b[0m       \u001b[32m0.8493\u001b[0m        \u001b[35m0.5548\u001b[0m  10.8040\n",
      "      3        \u001b[36m0.3365\u001b[0m       \u001b[32m0.8604\u001b[0m        \u001b[35m0.4987\u001b[0m  10.7130\n",
      "      4        \u001b[36m0.2907\u001b[0m       \u001b[32m0.8765\u001b[0m        \u001b[35m0.4667\u001b[0m  10.9925\n",
      "      5        \u001b[36m0.2614\u001b[0m       0.8739        \u001b[35m0.4495\u001b[0m  10.9734\n",
      "      6        \u001b[36m0.2408\u001b[0m       0.8759        \u001b[35m0.4318\u001b[0m  10.8021\n",
      "      7        \u001b[36m0.2253\u001b[0m       \u001b[32m0.8798\u001b[0m        \u001b[35m0.4129\u001b[0m  10.7509\n",
      "      8        \u001b[36m0.2121\u001b[0m       \u001b[32m0.8829\u001b[0m        \u001b[35m0.4044\u001b[0m  10.7450\n",
      "      9        \u001b[36m0.2017\u001b[0m       \u001b[32m0.8842\u001b[0m        0.4090  10.7140\n",
      "     10        \u001b[36m0.1928\u001b[0m       \u001b[32m0.8867\u001b[0m        \u001b[35m0.3954\u001b[0m  10.7290\n",
      "     11        \u001b[36m0.1853\u001b[0m       0.8846        \u001b[35m0.3942\u001b[0m  10.8840\n",
      "     12        \u001b[36m0.1783\u001b[0m       \u001b[32m0.8873\u001b[0m        0.3943  10.7740\n",
      "     13        \u001b[36m0.1723\u001b[0m       \u001b[32m0.8876\u001b[0m        \u001b[35m0.3902\u001b[0m  10.7260\n",
      "     14        \u001b[36m0.1669\u001b[0m       0.8862        \u001b[35m0.3890\u001b[0m  10.8093\n",
      "     15        \u001b[36m0.1617\u001b[0m       \u001b[32m0.8889\u001b[0m        0.3899  10.7310\n",
      "     16        \u001b[36m0.1565\u001b[0m       0.8870        0.3916  10.7310\n",
      "     17        \u001b[36m0.1533\u001b[0m       0.8877        \u001b[35m0.3871\u001b[0m  10.7210\n",
      "     18        \u001b[36m0.1495\u001b[0m       0.8868        0.3874  10.7290\n",
      "     19        \u001b[36m0.1456\u001b[0m       0.8867        0.3916  10.7200\n",
      "Stopping since valid_loss has not improved in the last 3 epochs.\n",
      "Training finished\n",
      "Saving results..\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "# train the transformer model\n",
    "(transformer_train_loss, \n",
    " transformer_valid_loss, \n",
    " transformer_valid_acc, \n",
    " transformer_test_acc, \n",
    " transformer_vocab_length) = train_test_dc(model_used='Transformer', batch_size=24, max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/transformer_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #transformer_train_loss = data['train_loss']\n",
    "    #transformer_valid_loss = data['valid_loss']\n",
    "    #transformer_valid_acc = data['valid_acc']\n",
    "    #transformer_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded\n",
      "Starting training..\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.3296\u001b[0m       \u001b[32m0.8072\u001b[0m        \u001b[35m0.7431\u001b[0m  10.8520\n",
      "      2        \u001b[36m0.4546\u001b[0m       \u001b[32m0.8406\u001b[0m        \u001b[35m0.5776\u001b[0m  10.9070\n",
      "      3        \u001b[36m0.3471\u001b[0m       \u001b[32m0.8629\u001b[0m        \u001b[35m0.5034\u001b[0m  10.9206\n",
      "      4        \u001b[36m0.2917\u001b[0m       \u001b[32m0.8716\u001b[0m        \u001b[35m0.4570\u001b[0m  10.7380\n",
      "      5        \u001b[36m0.2587\u001b[0m       \u001b[32m0.8819\u001b[0m        \u001b[35m0.4271\u001b[0m  10.7190\n",
      "      6        \u001b[36m0.2355\u001b[0m       \u001b[32m0.8862\u001b[0m        \u001b[35m0.4094\u001b[0m  10.7300\n",
      "      7        \u001b[36m0.2170\u001b[0m       0.8860        \u001b[35m0.4027\u001b[0m  10.7210\n",
      "      8        \u001b[36m0.2027\u001b[0m       \u001b[32m0.8900\u001b[0m        \u001b[35m0.3927\u001b[0m  10.7320\n",
      "      9        \u001b[36m0.1918\u001b[0m       0.8875        0.3968  10.7190\n",
      "     10        \u001b[36m0.1825\u001b[0m       \u001b[32m0.8908\u001b[0m        \u001b[35m0.3849\u001b[0m  10.7280\n",
      "     11        \u001b[36m0.1740\u001b[0m       0.8879        0.3870  10.7280\n",
      "     12        \u001b[36m0.1672\u001b[0m       \u001b[32m0.8926\u001b[0m        \u001b[35m0.3795\u001b[0m  10.7250\n",
      "     13        \u001b[36m0.1608\u001b[0m       \u001b[32m0.8933\u001b[0m        \u001b[35m0.3758\u001b[0m  10.7630\n",
      "     14        \u001b[36m0.1547\u001b[0m       \u001b[32m0.8937\u001b[0m        \u001b[35m0.3748\u001b[0m  10.7343\n",
      "     15        \u001b[36m0.1502\u001b[0m       0.8930        0.3807  10.7260\n",
      "     16        \u001b[36m0.1454\u001b[0m       0.8921        0.3849  10.7350\n",
      "Stopping since valid_loss has not improved in the last 3 epochs.\n",
      "Training finished\n",
      "Saving results..\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "# train the GPT2-Base model\n",
    "(gpt_base_train_loss, \n",
    " gpt_base_valid_loss, \n",
    " gpt_base_valid_acc, \n",
    " gpt_base_test_acc, \n",
    " gpt_base_vocab_length) = train_test_dc(model_used='GPT2_Base', batch_size=24, max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/gpt2_base_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #gpt_base_train_loss = data['train_loss']\n",
    "    #gpt_base_valid_loss = data['valid_loss']\n",
    "    #gpt_base_valid_acc = data['valid_acc']\n",
    "    #gpt_base_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded\n",
      "Starting training..\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.2958\u001b[0m       \u001b[32m0.8184\u001b[0m        \u001b[35m0.6594\u001b[0m  13.1779\n",
      "      2        \u001b[36m0.3964\u001b[0m       \u001b[32m0.8520\u001b[0m        \u001b[35m0.5211\u001b[0m  13.0480\n",
      "      3        \u001b[36m0.3063\u001b[0m       \u001b[32m0.8646\u001b[0m        \u001b[35m0.4618\u001b[0m  13.1145\n",
      "      4        \u001b[36m0.2597\u001b[0m       \u001b[32m0.8733\u001b[0m        \u001b[35m0.4301\u001b[0m  13.0390\n",
      "      5        \u001b[36m0.2297\u001b[0m       \u001b[32m0.8772\u001b[0m        \u001b[35m0.4205\u001b[0m  13.0410\n",
      "      6        \u001b[36m0.2072\u001b[0m       \u001b[32m0.8813\u001b[0m        \u001b[35m0.4048\u001b[0m  13.0430\n",
      "      7        \u001b[36m0.1906\u001b[0m       \u001b[32m0.8824\u001b[0m        \u001b[35m0.3991\u001b[0m  13.0320\n",
      "      8        \u001b[36m0.1778\u001b[0m       0.8813        \u001b[35m0.3943\u001b[0m  13.2603\n",
      "      9        \u001b[36m0.1674\u001b[0m       0.8817        0.3957  13.1200\n",
      "     10        \u001b[36m0.1578\u001b[0m       0.8820        0.3967  13.3691\n",
      "Stopping since valid_loss has not improved in the last 3 epochs.\n",
      "Training finished\n",
      "Saving results..\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "# train the GPT2-Medium model\n",
    "(gpt_medium_train_loss, \n",
    " gpt_medium_valid_loss, \n",
    " gpt_medium_valid_acc, \n",
    " gpt_medium_test_acc, \n",
    " gpt_medium_vocab_length) = train_test_dc(model_used='GPT2_Medium', batch_size=24, max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/gpt2_medium_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #gpt_medium_train_loss = data['train_loss']\n",
    "    #gpt_medium_valid_loss = data['valid_loss']\n",
    "    #gpt_medium_valid_acc = data['valid_acc']\n",
    "    #gpt_medium_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded\n",
      "Starting training..\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m5.3384\u001b[0m       \u001b[32m0.7452\u001b[0m        \u001b[35m2.8038\u001b[0m  9.6420\n",
      "      2        \u001b[36m1.3708\u001b[0m       \u001b[32m0.7898\u001b[0m        \u001b[35m1.3409\u001b[0m  9.5260\n",
      "      3        \u001b[36m0.7640\u001b[0m       \u001b[32m0.8118\u001b[0m        \u001b[35m0.9267\u001b[0m  9.5490\n",
      "      4        \u001b[36m0.5635\u001b[0m       \u001b[32m0.8233\u001b[0m        \u001b[35m0.7489\u001b[0m  9.5310\n",
      "      5        \u001b[36m0.4648\u001b[0m       \u001b[32m0.8321\u001b[0m        \u001b[35m0.6554\u001b[0m  9.8350\n",
      "      6        \u001b[36m0.4066\u001b[0m       \u001b[32m0.8394\u001b[0m        \u001b[35m0.5997\u001b[0m  9.5130\n",
      "      7        \u001b[36m0.3683\u001b[0m       \u001b[32m0.8451\u001b[0m        \u001b[35m0.5634\u001b[0m  9.5120\n",
      "      8        \u001b[36m0.3409\u001b[0m       \u001b[32m0.8491\u001b[0m        \u001b[35m0.5391\u001b[0m  9.5180\n",
      "      9        \u001b[36m0.3203\u001b[0m       \u001b[32m0.8525\u001b[0m        \u001b[35m0.5209\u001b[0m  9.5190\n",
      "     10        \u001b[36m0.3041\u001b[0m       \u001b[32m0.8549\u001b[0m        \u001b[35m0.5084\u001b[0m  9.4990\n",
      "     11        \u001b[36m0.2910\u001b[0m       \u001b[32m0.8554\u001b[0m        \u001b[35m0.4984\u001b[0m  9.5090\n",
      "     12        \u001b[36m0.2801\u001b[0m       \u001b[32m0.8567\u001b[0m        \u001b[35m0.4917\u001b[0m  9.5120\n",
      "     13        \u001b[36m0.2709\u001b[0m       \u001b[32m0.8580\u001b[0m        \u001b[35m0.4849\u001b[0m  9.8291\n",
      "     14        \u001b[36m0.2628\u001b[0m       \u001b[32m0.8588\u001b[0m        \u001b[35m0.4800\u001b[0m  9.5190\n",
      "     15        \u001b[36m0.2559\u001b[0m       \u001b[32m0.8598\u001b[0m        \u001b[35m0.4754\u001b[0m  9.5170\n",
      "     16        \u001b[36m0.2496\u001b[0m       \u001b[32m0.8605\u001b[0m        \u001b[35m0.4724\u001b[0m  9.5150\n",
      "     17        \u001b[36m0.2441\u001b[0m       \u001b[32m0.8613\u001b[0m        \u001b[35m0.4691\u001b[0m  9.5140\n",
      "     18        \u001b[36m0.2390\u001b[0m       \u001b[32m0.8614\u001b[0m        \u001b[35m0.4678\u001b[0m  9.5070\n",
      "     19        \u001b[36m0.2345\u001b[0m       \u001b[32m0.8617\u001b[0m        \u001b[35m0.4652\u001b[0m  9.5130\n",
      "     20        \u001b[36m0.2302\u001b[0m       \u001b[32m0.8621\u001b[0m        \u001b[35m0.4636\u001b[0m  9.5120\n",
      "     21        \u001b[36m0.2265\u001b[0m       \u001b[32m0.8631\u001b[0m        \u001b[35m0.4625\u001b[0m  9.5220\n",
      "     22        \u001b[36m0.2229\u001b[0m       \u001b[32m0.8631\u001b[0m        \u001b[35m0.4614\u001b[0m  9.5080\n",
      "     23        \u001b[36m0.2195\u001b[0m       \u001b[32m0.8638\u001b[0m        \u001b[35m0.4602\u001b[0m  9.5550\n",
      "     24        \u001b[36m0.2164\u001b[0m       0.8638        \u001b[35m0.4592\u001b[0m  9.7232\n",
      "     25        \u001b[36m0.2135\u001b[0m       0.8636        \u001b[35m0.4586\u001b[0m  9.6090\n",
      "     26        \u001b[36m0.2107\u001b[0m       0.8635        \u001b[35m0.4584\u001b[0m  9.5030\n",
      "     27        \u001b[36m0.2082\u001b[0m       \u001b[32m0.8641\u001b[0m        0.4584  9.7575\n",
      "     28        \u001b[36m0.2057\u001b[0m       0.8638        \u001b[35m0.4581\u001b[0m  9.5140\n",
      "     29        \u001b[36m0.2034\u001b[0m       \u001b[32m0.8641\u001b[0m        \u001b[35m0.4577\u001b[0m  9.5074\n",
      "     30        \u001b[36m0.2012\u001b[0m       \u001b[32m0.8649\u001b[0m        \u001b[35m0.4572\u001b[0m  9.5200\n",
      "     31        \u001b[36m0.1991\u001b[0m       \u001b[32m0.8650\u001b[0m        0.4575  9.5170\n",
      "     32        \u001b[36m0.1971\u001b[0m       \u001b[32m0.8651\u001b[0m        \u001b[35m0.4567\u001b[0m  9.5030\n",
      "     33        \u001b[36m0.1953\u001b[0m       \u001b[32m0.8654\u001b[0m        0.4574  9.5630\n",
      "     34        \u001b[36m0.1935\u001b[0m       0.8648        0.4577  9.5230\n",
      "Stopping since valid_loss has not improved in the last 3 epochs.\n",
      "Training finished\n",
      "Saving results..\n",
      "Results saved\n"
     ]
    }
   ],
   "source": [
    "# train the LSTM model\n",
    "(lstm_train_loss, \n",
    " lstm_valid_loss, \n",
    " lstm_valid_acc, \n",
    " lstm_test_acc, \n",
    " lstm_vocab_length) = train_test_dc(model_used='LSTM', batch_size=24, max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/lstm_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #lstm_train_loss = data['train_loss']\n",
    "    #lstm_valid_loss = data['valid_loss']\n",
    "    #lstm_valid_acc = data['valid_acc']\n",
    "    #lstm_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the results of the four different diagnostic classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilGPT-2 diagnostic classifier after convergence:\n",
      "Train loss = 0.1533\n",
      "Validation loss = 0.3871\n",
      "Validation accuracy = 0.8877\n",
      "Test accuracy = 0.8842\n"
     ]
    }
   ],
   "source": [
    "print('DistilGPT-2 diagnostic classifier after convergence:')\n",
    "print(f'Train loss = {transformer_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {transformer_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {transformer_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {transformer_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Base diagnostic classifier after convergence:\n",
      "Train loss = 0.1547\n",
      "Validation loss = 0.3748\n",
      "Validation accuracy = 0.8937\n",
      "Test accuracy = 0.8874\n"
     ]
    }
   ],
   "source": [
    "print('GPT-2 Base diagnostic classifier after convergence:')\n",
    "print(f'Train loss = {gpt_base_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {gpt_base_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {gpt_base_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {gpt_base_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Medium diagnostic classifier after convergence:\n",
      "Train loss = 0.1778\n",
      "Validation loss = 0.3943\n",
      "Validation accuracy = 0.8813\n",
      "Test accuracy = 0.8835\n"
     ]
    }
   ],
   "source": [
    "print('GPT-2 Medium diagnostic classifier after convergence:')\n",
    "print(f'Train loss = {gpt_medium_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {gpt_medium_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {gpt_medium_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {gpt_medium_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM diagnostic classifier after convergence:\n",
      "Train loss = 0.1971\n",
      "Validation loss = 0.4567\n",
      "Validation accuracy = 0.8651\n",
      "Test accuracy = 0.8660\n"
     ]
    }
   ],
   "source": [
    "print('LSTM diagnostic classifier after convergence:')\n",
    "print(f'Train loss = {lstm_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {lstm_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {lstm_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {lstm_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lstm_x = list(range(1, (len(lstm_train_loss[:-4]) + 1)))\n",
    "lstm_y = lstm_train_loss[:-4]\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_train_loss[:-4]) + 1)))\n",
    "transformer_y = transformer_train_loss[:-4]\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_train_loss[:-4]) + 1)))\n",
    "gpt_base_y = gpt_base_train_loss[:-4]\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_train_loss[:-4]) + 1)))\n",
    "gpt_medium_y = gpt_medium_train_loss[:-4]\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training loss')\n",
    "plt.title('Training loss of the diagnostic classifiers until convergence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_x = list(range(1, (len(lstm_valid_loss[:-4]) + 1)))\n",
    "lstm_y = lstm_valid_loss[:-4]\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_valid_loss[:-4]) + 1)))\n",
    "transformer_y = transformer_valid_loss[:-4]\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_valid_loss[:-4]) + 1)))\n",
    "gpt_base_y = gpt_base_valid_loss[:-4]\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_valid_loss[:-4]) + 1)))\n",
    "gpt_medium_y = gpt_medium_valid_loss[:-4]\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Validation loss of the diagnostic classifiers until convergence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_x = list(range(1, (len(lstm_valid_acc[:-4]) + 1)))\n",
    "lstm_y = lstm_valid_acc[:-4]\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_valid_acc[:-4]) + 1)))\n",
    "transformer_y = transformer_valid_acc[:-4]\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_valid_acc[:-4]) + 1)))\n",
    "gpt_base_y = gpt_base_valid_acc[:-4]\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_valid_acc[:-4]) + 1)))\n",
    "gpt_medium_y = gpt_medium_valid_acc[:-4]\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.title('Validation accuracy of the diagnostic classifiers until convergence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "We see that the Diagnostic Classifier performs very well (>85% validation and test accuracy) on the representations of all models. This indicates that both the LSTM and Transformer encode linguistic features into their representations. However, we see that Transformer models performer slightly better than LSTM models.\n",
    "\n",
    "Interestingly, we see no correlation between model size and Diagnostic Classifier performance for the different Transformer models. This seems to indicate that there is no relation between model size and the extent to which Transformer models are able to capture linguistic features like POS-tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating probing results with control tasks\n",
    "In this section we will examine the faithfulness of the probing results using control tasks. We do this by creating a control task that predicts (pos_tag, token) pairs. We then train a Diagnostic Classifier on the hidden states of the Transformer model and the LSTM model with this control task.\n",
    "\n",
    "We start by creating a control task. Then, we train our Diagnostic Classifier with this task on the encodings from the GPT-2 transformer and the encodings from the Gulordava LSTM model. Lastly, we compare this with the performance on the real task and calculate the selectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a function that generates a new vocabulary that contains (pos_tag, token) pairs instead of a vocabulary that is solely based on the POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# function that fetches the control tags\n",
    "def fetch_control_tags(ud_parses: List[TokenList], pos_vocab_length, control_vocab=None) -> Tensor:\n",
    "    # check if the vocabulary is None\n",
    "    if control_vocab is None:\n",
    "        # create a new vocab\n",
    "        control_vocab = defaultdict()\n",
    "    \n",
    "    # create a list of tags from the input\n",
    "    all_tags = []\n",
    "    \n",
    "    # loop over the parsed sentences\n",
    "    for sentence in ud_parses:    \n",
    "        # loop over the words in the parsed sentence\n",
    "        for word in sentence:\n",
    "            # get the token of the word\n",
    "            token = word['form']\n",
    "            \n",
    "            # check if the token does not exist in the vocabulary\n",
    "            if token not in control_vocab:\n",
    "                # create a new entry, which is a random number in the range of the pos_vocab_length\n",
    "                control_vocab[token] = random.randrange(pos_vocab_length)\n",
    "            \n",
    "            # add tag to the tags list\n",
    "            all_tags.append(torch.tensor(control_vocab[token]))\n",
    "    \n",
    "    # stack the tags into a tensor\n",
    "    all_tags = torch.stack(all_tags, dim=0)\n",
    "    \n",
    "    # return the tags and vocabulary\n",
    "    return all_tags, control_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we create a new version of our data creation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates the control data\n",
    "def create_control_data(filename: str, lm, w2i, pos_vocab_length, pos_vocab=None):\n",
    "    ud_parses = parse_corpus(filename)\n",
    "    \n",
    "    sen_reps = fetch_sen_reps(ud_parses, lm, w2i)\n",
    "    pos_tags, pos_vocab = fetch_control_tags(ud_parses, pos_vocab_length, control_vocab=pos_vocab)\n",
    "    \n",
    "    return sen_reps, pos_tags, pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the correct control data for each of the models\n",
    "def create_pos_control_data(pos_vocab_length, model_used='Transformer'):\n",
    "    # check which model to use\n",
    "    if (model_used == 'Transformer'):\n",
    "        lm = transformer_model\n",
    "        w2i = transformer_tokenizer\n",
    "    elif (model_used == 'GPT2_Base'):\n",
    "        lm = gpt_base_model\n",
    "        w2i = gpt_base_tokenizer\n",
    "    elif (model_used == 'GPT2_Medium'):\n",
    "        lm = gpt_medium_model\n",
    "        w2i = gpt_medium_tokenizer\n",
    "    else:\n",
    "        lm = lstm\n",
    "        w2i = vocab\n",
    "    use_sample = True\n",
    "\n",
    "    train_x, train_y, train_vocab = create_control_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "        lm, \n",
    "        w2i,\n",
    "        pos_vocab_length\n",
    "    )\n",
    "\n",
    "    dev_x, dev_y, _ = create_control_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "        lm, \n",
    "        w2i,\n",
    "        pos_vocab_length,\n",
    "        pos_vocab=train_vocab\n",
    "    )\n",
    "\n",
    "    test_x, test_y, _ = create_control_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "        lm,\n",
    "        w2i,\n",
    "        pos_vocab_length,\n",
    "        pos_vocab=train_vocab\n",
    "    )\n",
    "    \n",
    "    # return the data\n",
    "    return train_x, train_y, dev_x, dev_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "\n",
    "# function that trains the DC on the control task and returns the metrics\n",
    "def train_test_control_dc(vocab_length, model_used='Transformer', batch_size=24, max_epochs=2, save_results=True):\n",
    "    # get the data for the model\n",
    "    print('Loading data..')\n",
    "    train_x, train_y, dev_x, dev_y, test_x, test_y = create_pos_control_data(vocab_length, model_used)\n",
    "    print('Data loaded')\n",
    "    \n",
    "    # combine train and validation datasets\n",
    "    x_train = torch.cat((train_x, dev_x), dim=0)\n",
    "    y_train = torch.cat((train_y, dev_y), dim=0)\n",
    "    \n",
    "    # train the diagnostic classifier\n",
    "    print('Starting training..')\n",
    "    trained_dc = train_dc(DiagnosticClassifier, x_train.cpu(), y_train.cpu(), device, batch_size=batch_size, max_epochs=max_epochs)\n",
    "    print('Training finished')\n",
    "    \n",
    "    # get the DC data\n",
    "    train_loss = trained_dc.history[:, 'train_loss']\n",
    "    valid_loss = trained_dc.history[:, 'valid_loss']\n",
    "    valid_acc = trained_dc.history[:, 'valid_acc']\n",
    "    \n",
    "    # run the test data through the model\n",
    "    test_preds = trained_dc.predict(test_x.cpu())\n",
    "    test_acc = accuracy_score(test_y.cpu(), test_preds)\n",
    "    \n",
    "    if save_results:\n",
    "        # save the results\n",
    "        print('Saving results..')\n",
    "        data = {}\n",
    "        data['train_loss'] = train_loss\n",
    "        data['valid_loss'] = valid_loss\n",
    "        data['valid_acc'] = valid_acc\n",
    "        data['test_acc'] = test_acc\n",
    "        with open('model_results/' + model_used.lower() + '_control_dc.txt', 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        print('Results saved')\n",
    "    \n",
    "    # return the measures \n",
    "    return train_loss, valid_loss, valid_acc, test_acc, vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our Diagnostic Classifier on the new control task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the transformer model\n",
    "(transformer_control_train_loss, \n",
    " transformer_control_valid_loss, \n",
    " transformer_control_valid_acc, \n",
    " transformer_control_test_acc, \n",
    " transformer_control_vocab_length) = train_test_control_dc(transformer_vocab_length, \n",
    "                                                           model_used='Transformer', \n",
    "                                                           batch_size=24, \n",
    "                                                           max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/transformer_control_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #transformer_control_train_loss = data['train_loss']\n",
    "    #transformer_control_valid_loss = data['valid_loss']\n",
    "    #transformer_control_valid_acc = data['valid_acc']\n",
    "    #transformer_control_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GPT-2 Base model\n",
    "(gpt_base_control_train_loss, \n",
    " gpt_base_control_valid_loss, \n",
    " gpt_base_control_valid_acc, \n",
    " gpt_base_control_test_acc, \n",
    " gpt_base_control_vocab_length) = train_test_control_dc(gpt_base_vocab_length, \n",
    "                                                        model_used='GPT_Base', \n",
    "                                                        batch_size=24, \n",
    "                                                        max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/gpt_base_control_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #gpt_base_control_train_loss = data['train_loss']\n",
    "    #gpt_base_control_valid_loss = data['valid_loss']\n",
    "    #gpt_base_control_valid_acc = data['valid_acc']\n",
    "    #gpt_base_control_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GPT-2 Medium model\n",
    "(gpt_medium_control_train_loss, \n",
    " gpt_medium_control_valid_loss, \n",
    " gpt_medium_control_valid_acc, \n",
    " gpt_medium_control_test_acc, \n",
    " gpt_medium_control_vocab_length) = train_test_control_dc(gpt_medium_vocab_length, \n",
    "                                                          model_used='GPT_Medium', \n",
    "                                                          batch_size=24, \n",
    "                                                          max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/gpt_medium_control_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #gpt_medium_control_train_loss = data['train_loss']\n",
    "    #gpt_medium_control_valid_loss = data['valid_loss']\n",
    "    #gpt_medium_control_valid_acc = data['valid_acc']\n",
    "    #gpt_medium_control_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the LSTM model\n",
    "(lstm_control_train_loss, \n",
    " lstm_control_valid_loss, \n",
    " lstm_control_valid_acc, \n",
    " lstm_control_test_acc, \n",
    " lstm_control_vocab_length) = train_test_control_dc(lstm_vocab_length, \n",
    "                                                    model_used='LSTM', \n",
    "                                                    batch_size=24, \n",
    "                                                    max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/lstm_control_dc.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #lstm_control_train_loss = data['train_loss']\n",
    "    #lstm_control_valid_loss = data['valid_loss']\n",
    "    #lstm_control_valid_acc = data['valid_acc']\n",
    "    #lstm_control_test_acc = data['test_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the results of the four different diagnostic classifiers on the control task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DistilGPT-2 diagnostic classifier on control task after convergence:')\n",
    "print(f'Train loss = {transformer_control_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {transformer_control_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {transformer_control_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {transformer_control_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPT-2 Base diagnostic classifier on control task after convergence:')\n",
    "print(f'Train loss = {gpt_base_control_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {gpt_base_control_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {gpt_base_control_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {gpt_base_control_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPT-2 Medium diagnostic classifier on control task after convergence:')\n",
    "print(f'Train loss = {gpt_medium_control_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {gpt_medium_control_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {gpt_medium_control_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {gpt_medium_control_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM diagnostic classifier on control task after convergence:')\n",
    "print(f'Train loss = {lstm_control_train_loss[-4]:.4f}')\n",
    "print(f'Validation loss = {lstm_control_valid_loss[-4]:.4f}')\n",
    "print(f'Validation accuracy = {lstm_control_valid_acc[-4]:.4f}')\n",
    "print(f'Test accuracy = {lstm_control_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lstm_x = list(range(1, (len(lstm_control_train_loss[:-4]) + 1)))\n",
    "lstm_y = lstm_control_train_loss[:-4]\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_control_train_loss[:-4]) + 1)))\n",
    "transformer_y = transformer_control_train_loss[:-4]\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_control_train_loss[:-4]) + 1)))\n",
    "gpt_base_y = gpt_base_control_train_loss[:-4]\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_control_train_loss[:-4]) + 1)))\n",
    "gpt_medium_y = gpt_medium_control_train_loss[:-4]\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training loss')\n",
    "plt.title('Training loss of the diagnostic classifiers on control task until convergence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_x = list(range(1, (len(lstm_control_valid_loss[:-4]) + 1)))\n",
    "lstm_y = lstm_control_valid_loss[:-4]\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_control_valid_loss[:-4]) + 1)))\n",
    "transformer_y = transformer_control_valid_loss[:-4]\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_control_valid_loss[:-4]) + 1)))\n",
    "gpt_base_y = gpt_base_control_valid_loss[:-4]\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_control_valid_loss[:-4]) + 1)))\n",
    "gpt_medium_y = gpt_medium_control_valid_loss[:-4]\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Validation loss of the diagnostic classifiers on control task until convergence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_x = list(range(1, (len(lstm_control_valid_acc[:-4]) + 1)))\n",
    "lstm_y = lstm_control_valid_acc[:-4]\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_control_valid_acc[:-4]) + 1)))\n",
    "transformer_y = transformer_control_valid_acc[:-4]\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_control_valid_acc[:-4]) + 1)))\n",
    "gpt_base_y = gpt_base_control_valid_acc[:-4]\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_control_valid_acc[:-4]) + 1)))\n",
    "gpt_medium_y = gpt_medium_control_valid_acc[:-4]\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.title('Validation accuracy of the diagnostic classifiers on control task until convergence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at the selectivity of our Diagnostic Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DistilGPT-2 diagnostic classifier:')\n",
    "print(f'Real task test accuracy = {transformer_test_acc:.4f}')\n",
    "print(f'Control task test accuracy = {transformer_control_test_acc:.4f}')\n",
    "print(f'Selectivity = {(transformer_test_acc - transformer_control_test_acc):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPT-2 Base diagnostic classifier:')\n",
    "print(f'Real task test accuracy = {gpt_base_test_acc:.4f}')\n",
    "print(f'Control task test accuracy = {gpt_base_control_test_acc:.4f}')\n",
    "print(f'Selectivity = {(gpt_base_test_acc - gpt_base_control_test_acc):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPT-2 Medium diagnostic classifier:')\n",
    "print(f'Real task test accuracy = {gpt_medium_test_acc:.4f}')\n",
    "print(f'Control task test accuracy = {gpt_medium_control_test_acc:.4f}')\n",
    "print(f'Selectivity = {(gpt_medium_test_acc - gpt_medium_control_test_acc):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM diagnostic classifier:')\n",
    "print(f'Real task test accuracy = {lstm_test_acc:.4f}')\n",
    "print(f'Control task test accuracy = {lstm_control_test_acc:.4f}')\n",
    "print(f'Selectivity = {(lstm_test_acc - lstm_control_test_acc):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "We see that the Diagnostic Classifier performs very poorly on the control task (<50% validation and test accuracy) on the representations of all models. This indicates that the good performance on the real task does not come from the fact that the Diagnostic Classifier is strong enough to learn any mapping. This is backed up by the high selectivity that we see for all Diagnostic Classifiers (around 35-40%). Therefore, the good performance on the real task comes from the fact that the Transformer and LSTM models are able to capture linguistic features like POS-tags in their hidden states.\n",
    "\n",
    "Interestingly, we see that the DistilGPT-2 model is the best performer while it has the fewest parameters of the Transformers. The bigger models (GPT2 Base and Medium) show a lower selectivity. This comes from the fact that the bigger models perform worse (than DistilGPT-2) on the real task but better (than DistilGPT-2) on the control task. This seems to indicate that model size negatively impacts selectivity of the Diagnostic Classifier.\n",
    "\n",
    "**CONCLUSIE IS NOG NIET DEFINITIEF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notion of syntactic structure\n",
    "In this section we will examine whether transformer models have a stronger notion of syntactic structure than recurrent models? We do this by applying our structural probe on the representations of our four different models (DistilGPT-2, GPT-2 Base, GPT-2 Medium, and LSTM).\n",
    "\n",
    "We start by training our structural probe on the encodings of the different models. We then compare the performance of the probe on both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the results\n",
    "def save_probe_results(name, train_loss, valid_loss, valid_uuas, test_loss, test_uuas):\n",
    "    print('Saving results..')\n",
    "    data = {}\n",
    "    data['train_loss'] = train_loss\n",
    "    data['valid_loss'] = valid_loss\n",
    "    data['valid_uuas'] = valid_uuas\n",
    "    data['test_loss'] = test_loss\n",
    "    data['test_uuas'] = test_uuas\n",
    "    with open('model_results/' + name + '_probe.txt', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print('Results saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets for DistilGPT-2\n",
    "train_y, train_x = init_corpus('data/en_ewt-ud-train.conllu', transformer_model, transformer_tokenizer)\n",
    "dev_y, dev_x = init_corpus('data/en_ewt-ud-dev.conllu', transformer_model, transformer_tokenizer)\n",
    "test_y, test_x = init_corpus('data/en_ewt-ud-test.conllu', transformer_model, transformer_tokenizer)\n",
    "\n",
    "# train the probe for DistilGPT-2\n",
    "(_, transformer_train_loss, \n",
    " transformer_valid_loss, \n",
    " transformer_valid_uuas, \n",
    " transformer_test_loss, \n",
    " transformer_test_uuas) = train_probe(train_x, train_y, dev_x, dev_y, test_x, test_y, emb_dim=768, batch_size=24, epochs=50)\n",
    "\n",
    "# save the results\n",
    "save_probe_results('transformer', transformer_train_loss, transformer_valid_loss, transformer_valid_uuas, \n",
    "                   transformer_test_loss, transformer_test_uuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/transformer_probe.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #transformer_train_loss = data['train_loss']\n",
    "    #transformer_valid_loss = data['valid_loss']\n",
    "    #transformer_valid_uuas = data['valid_uuas']\n",
    "    #transformer_test_loss = data['test_loss']\n",
    "    #transformer_test_uuas = data['test_uuas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets for GPT-2 Base\n",
    "train_y, train_x = init_corpus('data/en_ewt-ud-train.conllu', gpt_base_model, gpt_base_tokenizer)\n",
    "dev_y, dev_x = init_corpus('data/en_ewt-ud-dev.conllu', gpt_base_model, gpt_base_tokenizer)\n",
    "test_y, test_x = init_corpus('data/en_ewt-ud-test.conllu', gpt_base_model, gpt_base_tokenizer)\n",
    "\n",
    "# train the probe for GPT-2 Base\n",
    "(_, gpt_base_train_loss, \n",
    " gpt_base_valid_loss, \n",
    " gpt_base_valid_uuas, \n",
    " gpt_base_test_loss, \n",
    " gpt_base_test_uuas) = train_probe(train_x, train_y, dev_x, dev_y, test_x, test_y, emb_dim=768, batch_size=24, epochs=50)\n",
    "\n",
    "# save the results\n",
    "save_probe_results('gpt2_base', gpt_base_train_loss, gpt_base_valid_loss, gpt_base_valid_uuas, \n",
    "                   gpt_base_test_loss, gpt_base_test_uuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/gpt2_base_probe.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #gpt_base_train_loss = data['train_loss']\n",
    "    #gpt_base_valid_loss = data['valid_loss']\n",
    "    #gpt_base_valid_uuas = data['valid_uuas']\n",
    "    #gpt_base_test_loss = data['test_loss']\n",
    "    #gpt_base_test_uuas = data['test_uuas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets for GPT-2 Medium\n",
    "train_y, train_x = init_corpus('data/en_ewt-ud-train.conllu', gpt_medium_model, gpt_medium_tokenizer)\n",
    "dev_y, dev_x = init_corpus('data/en_ewt-ud-dev.conllu', gpt_medium_model, gpt_medium_tokenizer)\n",
    "test_y, test_x = init_corpus('data/en_ewt-ud-test.conllu', gpt_medium_model, gpt_medium_tokenizer)\n",
    "\n",
    "# train the probe for GPT-2 Medium\n",
    "(_, gpt_medium_train_loss, \n",
    " gpt_medium_valid_loss, \n",
    " gpt_medium_valid_uuas, \n",
    " gpt_medium_test_loss, \n",
    " gpt_medium_test_uuas) = train_probe(train_x, train_y, dev_x, dev_y, test_x, test_y, emb_dim=1024, batch_size=24, epochs=50)\n",
    "\n",
    "# save the results\n",
    "save_probe_results('gpt2_medium', gpt_medium_train_loss, gpt_medium_valid_loss, gpt_medium_valid_uuas, \n",
    "                   gpt_medium_test_loss, gpt_medium_test_uuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/gpt2_medium_probe.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #gpt_medium_train_loss = data['train_loss']\n",
    "    #gpt_medium_valid_loss = data['valid_loss']\n",
    "    #gpt_medium_valid_uuas = data['valid_uuas']\n",
    "    #gpt_medium_test_loss = data['test_loss']\n",
    "    #gpt_medium_test_uuas = data['test_uuas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets for LSTM\n",
    "train_y, train_x = init_corpus('data/en_ewt-ud-train.conllu', lstm, w2i)\n",
    "dev_y, dev_x = init_corpus('data/en_ewt-ud-dev.conllu', lstm, w2i)\n",
    "test_y, test_x = init_corpus('data/en_ewt-ud-test.conllu', lstm, w2i)\n",
    "\n",
    "# train the probe for LSTM\n",
    "(_, lstm_train_loss, \n",
    " lstm_valid_loss, \n",
    " lstm_valid_uuas, \n",
    " lstm_test_loss, \n",
    " lstm_test_uuas) = train_probe(train_x, train_y, dev_x, dev_y, test_x, test_y, emb_dim=650, batch_size=24, epochs=50)\n",
    "\n",
    "# save the results\n",
    "save_probe_results('lstm', lstm_train_loss, lstm_valid_loss, lstm_valid_uuas, \n",
    "                   lstm_test_loss, lstm_test_uuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# or load the results from an earlier run\n",
    "#with open('model_results/lstm_probe.txt') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "    #lstm_train_loss = data['train_loss']\n",
    "    #lstm_valid_loss = data['valid_loss']\n",
    "    #lstm_valid_uuas = data['valid_uuas']\n",
    "    #lstm_test_loss = data['test_loss']\n",
    "    #lstm_test_uuas = data['test_uuas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the results of the four different structural probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DistilGPT-2 structural probe after 50 epochs:')\n",
    "print(f'Train loss = {transformer_train_loss[-1]:.4f}')\n",
    "print(f'Validation loss = {transformer_valid_loss[-1]:.4f}')\n",
    "print(f'Validation UUAS = {transformer_valid_uuas[-1]:.4f}')\n",
    "print(f'Test loss = {transformer_test_loss:.4f}')\n",
    "print(f'Test UUAS = {transformer_test_uuas:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPT-2 Base structural probe after 50 epochs:')\n",
    "print(f'Train loss = {gpt_base_train_loss[-1]:.4f}')\n",
    "print(f'Validation loss = {gpt_base_valid_loss[-1]:.4f}')\n",
    "print(f'Validation UUAS = {gpt_base_valid_uuas[-1]:.4f}')\n",
    "print(f'Test loss = {gpt_base_test_loss:.4f}')\n",
    "print(f'Test UUAS = {gpt_base_test_uuas:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPT-2 Medium structural probe after 50 epochs:')\n",
    "print(f'Train loss = {gpt_medium_train_loss[-1]:.4f}')\n",
    "print(f'Validation loss = {gpt_medium_valid_loss[-1]:.4f}')\n",
    "print(f'Validation UUAS = {gpt_medium_valid_uuas[-1]:.4f}')\n",
    "print(f'Test loss = {gpt_medium_test_loss:.4f}')\n",
    "print(f'Test UUAS = {gpt_medium_test_uuas:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM structural probe after 50 epochs:')\n",
    "print(f'Train loss = {lstm_train_loss[-1]:.4f}')\n",
    "print(f'Validation loss = {lstm_valid_loss[-1]:.4f}')\n",
    "print(f'Validation UUAS = {lstm_valid_uuas[-1]:.4f}')\n",
    "print(f'Test loss = {lstm_test_loss:.4f}')\n",
    "print(f'Test UUAS = {lstm_test_uuas:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lstm_x = list(range(1, (len(lstm_train_loss) + 1)))\n",
    "lstm_y = lstm_train_loss\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_train_loss) + 1)))\n",
    "transformer_y = transformer_train_loss\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_train_loss) + 1)))\n",
    "gpt_base_y = gpt_base_train_loss\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_train_loss) + 1)))\n",
    "gpt_medium_y = gpt_medium_train_loss\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training loss')\n",
    "plt.title('Training loss of the structural probes after 50 epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_x = list(range(1, (len(lstm_valid_loss) + 1)))\n",
    "lstm_y = lstm_valid_loss\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_valid_loss) + 1)))\n",
    "transformer_y = transformer_valid_loss\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_valid_loss) + 1)))\n",
    "gpt_base_y = gpt_base_valid_loss\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_valid_loss) + 1)))\n",
    "gpt_medium_y = gpt_medium_valid_loss\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Validation loss of the structural probes after 50 epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_x = list(range(1, (len(lstm_valid_uuas) + 1)))\n",
    "lstm_y = lstm_valid_uuas\n",
    "plt.plot(lstm_x, lstm_y, label = \"LSTM\")\n",
    "\n",
    "transformer_x = list(range(1, (len(transformer_valid_uuas) + 1)))\n",
    "transformer_y = transformer_valid_uuas\n",
    "plt.plot(transformer_x, transformer_y, label = \"DistilGPT-2\")\n",
    "\n",
    "gpt_base_x = list(range(1, (len(gpt_base_valid_uuas) + 1)))\n",
    "gpt_base_y = gpt_base_valid_uuas\n",
    "plt.plot(gpt_base_x, gpt_base_y, label = \"GPT-2 Base\")\n",
    "\n",
    "gpt_medium_x = list(range(1, (len(gpt_medium_valid_uuas) + 1)))\n",
    "gpt_medium_y = gpt_medium_valid_uuas\n",
    "plt.plot(gpt_medium_x, gpt_medium_y, label = \"GPT-2 Medium\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation UUAS')\n",
    "plt.title('Validation UUAS of the structural probes after 50 epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "**CONCLUSIE SCHRIJVEN**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
